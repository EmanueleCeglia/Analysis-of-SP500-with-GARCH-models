---
title: "Homework 4 Adv Ts"
author: "Emanuele Ceglia, Paolo Lorusso"
date: '2022-06-07'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, include=FALSE}
library(quantmod)
library(dygraphs)
library(rugarch)
library(PerformanceAnalytics)
library(stats)
```




```{r, results='hide'}
tickers <- c("^GSPC")
start_stream <- '2015-01-01'
end_stream = Sys.Date()

getSymbols(tickers, src = 'yahoo', from = start_stream, to=end_stream) 

GSPC_week <- to.weekly(GSPC)
GSPC_week_rtn <- diff(log(GSPC_week$GSPC.Close))
GSPC_week_rtn <- na.omit(GSPC_week_rtn)
```
<p>Below we can see the price chart of the Standard and Poor 500, is a stock market index tracking the performance of 500 large companies listed on stock exchanges in the United States. It is one of the most commonly followed equity indices.<br><br>
According to the definition of volatility clustering: <i>"Large changes tend to be followed by large changes, and small changes tend to be followed by small changes. "</i> We have underlined some periods of high volatility, often characterized by negative trends, using vertical lines.  </p>


```{r, echo=FALSE}
SP500 <- GSPC_week$GSPC.Adjusted
dygraph(SP500, main="SP500 Close prices") %>%
  dySeries(name="GSPC.Adjusted", label = "Close Price") %>%
  dyEvent("2015-07-31", "Start", labelLoc = "top") %>%
  dyEvent("2016-04-15", "End", labelLoc = "top") %>%
  dyEvent("2018-01-05", "Start", labelLoc = "top") %>%
  dyEvent("2019-01-18", "End", labelLoc = "top") %>%
  dyEvent("2020-01-17", "Start", labelLoc = "top") %>%
  dyEvent("2020-12-18", "End", labelLoc = "top") %>%
  dyEvent("2021-12-31", "Start", labelLoc = "top") %>%
  dyEvent("2022-06-22", "End", labelLoc = "top") %>%
  dyAxis("y", label = "Prices") %>%
  dyOptions(stackedGraph = TRUE) %>%
  dyRangeSelector(height = 30)
```


```{r, echo=FALSE, results='hide'}
s <- ugarchspec(mean.model = list(armaOrder = c(0,0)),
                variance.model = list(model = 'sGARCH'),
                distribution.model = 'norm')

m <- ugarchfit(data = GSPC_week_rtn, spec = s)
```

<p><br>For our analysis we have to use log returns because they satisfy some stylized facts.<br>
1) Returns are either serially uncorrelated or weakly dependent over time.<br>
2) Returns are not independent: square and absolute returns show correlation (even at high lags)<br>
3) Volatility is an important feature of asset returns. It changes
slowly over time and thus it shows up in the ACF of absolute or
squared returns.<br>
4) Financial time series exhibit volatility clustering. The conditional
variance is time varying and periods where volatility is low are
alternated by periods of high volatility.<br><br>
We can observe the first two facts below</p>
```{r, echo=FALSE}
par(mfrow=c(1,3))
plot(m, which=4)
plot(m, which=5)
plot(m, which=6)
```

<p>In the first graph we can see the autocorrelation function of log returns, they are significantly uncorrelated.<br>
While in the second and third graph we can see the acf of squared and absolute log returns, as told in 2) they are not independent, in fact there is a positive and significant correlation from lag of order one till 6/10<br></p>
<p>Moreover, using log returns: log(Pt/Pt-1) the conditional and unconditional mean return is zero so our series becomes yt = εt = σt|t−1*zt, where Zt ∼ NID(0, 1)<br>
hence, yt is a martingale difference sequence and therefore<br> 
- E(yt) = 0.<br>
- E(y<sup>2</sup>t|Ft−1) = E(σ<sup>2</sup>t|t−1z<sup>2</sup>t|Ft−1) = σ<sup>2</sup>t|t−1E(z<sup>2</sup>t|Ft−1) = σ<sup>2</sup>t|t−1E(z<sup>2</sup>t) = σ<sup>2</sup>t|t−1  (as shown in volatility chart) <br>
</p>

```{r, echo=FALSE}
SP500_rtn <- GSPC_week_rtn
dygraph(SP500_rtn, main="SP500 log-returns") %>%
  dySeries(name="GSPC.Close", label = "Log-Ret") %>%
  dyEvent("2015-07-31", "Start", labelLoc = "top") %>%
  dyEvent("2016-04-15", "End", labelLoc = "top") %>%
  dyEvent("2018-01-05", "Start", labelLoc = "top") %>%
  dyEvent("2019-01-18", "End", labelLoc = "top") %>%
  dyEvent("2020-01-17", "Start", labelLoc = "top") %>%
  dyEvent("2020-12-18", "End", labelLoc = "top") %>%
  dyEvent("2021-12-31", "Start", labelLoc = "top") %>%
  dyEvent("2022-06-22", "End", labelLoc = "top") %>%
  dyAxis("y", label = "log(Pt/Pt-1)") %>%
  dyOptions(stackedGraph = TRUE) %>%
  dyRangeSelector(height = 30)
```



<p><br>
For a better volatility clustering is usefull the chart of variance, in fact it's easy to see periods of high volatility between the vertical lines<p>


```{r, echo=FALSE}
SP500_rtn <- GSPC_week_rtn
dygraph(SP500_rtn^2, main="SP500 volatility") %>%
  dySeries(name="GSPC.Close", label = "Variance") %>%
  dyEvent("2015-07-31", "Start", labelLoc = "top") %>%
  dyEvent("2016-04-15", "End", labelLoc = "top") %>%
  dyEvent("2018-01-05", "Start", labelLoc = "top") %>%
  dyEvent("2019-01-18", "End", labelLoc = "top") %>%
  dyEvent("2020-01-17", "Start", labelLoc = "top") %>%
  dyEvent("2020-12-18", "End", labelLoc = "top") %>%
  dyEvent("2021-12-31", "Start", labelLoc = "top") %>%
  dyEvent("2022-06-22", "End", labelLoc = "top") %>%
  dyAxis("y", label = "log(Pt/Pt-1)^2") %>%
  dyOptions(stackedGraph = TRUE) %>%
  dyRangeSelector(height = 30)
```


<p><br><br>Define GARCH(1,1) model with <b>normal distribution assumpution for errors</b></p>

```{r, results='hide'}
s <- ugarchspec(mean.model = list(armaOrder = c(0,0)),
                variance.model = list(model = 'sGARCH'),
                distribution.model = 'norm')

m <- ugarchfit(data = GSPC_week_rtn, spec = s)
```
```{r, echo=FALSE}
print(capture.output(m)[2:19])
```

<p>As we can see, the parameters estimated are all positive and significantly relevant and as studied alpha is "near" to zero while beta is "near" to one.<br><br>
Now is important to verify the white noise assumption for residuals, in order to do this we can use the two tests below. All the p-value (for different lags) are higher than 0.05 so we can accept the null hypotesis of 'no serial correlation'.</p>

```{r, echo=FALSE}
capture.output(m)[37:52]
```

<p>We can see it also in these two graphs.</p>

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot(m, which=10)
plot(m, which=11)
```

<p>
At the end, we can verify how good is our assumption about the normality distribution of errors, high p-values means that our assumption is correct. In this case they are slightly higher than 0.05.<br>
We can see it also in the qq-plot below. But we can notice that for negative returns sample quantiles do not overlap the theoretical line.<br>
Moreover, in the second graph below is shown the histogram of log-returns, we can deduce the presence of a negative skweness and a kurtosis. Making our normality assumption of errors inaccurate.  
</p>
```{r, echo=FALSE}
cat('Skweness:', skewness(GSPC_week_rtn),'   Kurtosis:', kurtosis(GSPC_week_rtn))
```

```{r, echo=FALSE}
print(capture.output(m)[83:89])
```

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot(m, which=9)
hist(SP500_rtn, freq = TRUE, main="Distribution of SP500 index", breaks = 50) 
points(density(SP500_rtn), type="l", col="blue")
```

<p>
The normality assumption for errors is more appropriate for analyze periods of economic stability, because in these periods the stock market tends to be less volatile.<br>
In our case the time period considered starts in 2015 and ends today, as shown in the volatility clustering we have found some periods of high stress. These periods are generated by exogenous factors. In particular:<br> 
in the second period investors feared a central bank ready to tighten monetary policy, a slowing economy, and an intensifying trade war between the U.S. and China;<br>
in the third period there was the outbreak of Covid-19;<br>
while in the fourth period we can see the effects of the war between Russia and Ukraine.<br>
All these events underline the fact that we are analyzing a period of economic instability, with frequent market drops which increase market volatility.<br>
So, we can try a different distribution for innovation, and keeping in mind the values of skweness and kurtosis, our proposal is to use a skew Student-t distribution.

<hr>


<p><br><br>Define GARCH(1,1) model with <b>Skew Student-t distribution assumpution for errors</b></p>

```{r, results='hide'}
s <- ugarchspec(mean.model = list(armaOrder = c(0,0)),
                variance.model = list(model = 'sGARCH'),
                distribution.model = 'sstd')

m <- ugarchfit(data = GSPC_week_rtn, spec = s)
```
```{r, echo=FALSE}
print(capture.output(m)[2:20])
```

<p>Also in this case, we can accept that our parameters ar all significant.</p>

```{r, echo=FALSE}
capture.output(m)[41:56]
```

<p>There is no serial correlation, so our residuals follow a white noise process</p>

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot(m, which=10)
plot(m, which=11)
```

<p>But the most interesting part is shown in the final test. Now we can see that our assumption of a skew Student-t distribution is very accurate.</p>

```{r, echo=FALSE}
capture.output(m)[89:95]
```

<p>Now our sample quantiles overlap the theoretical quantiles more accurately than before.<br>
We can see an outlier on the bottom left side, we are preatty sure that is related to the 20 march 2020, on that day the SP500, due to the Covid-19 outbreak, plummed to 2.300$ making a negative performance of 30%.  </p>

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot(m, which=9)
hist(SP500_rtn, freq = TRUE, main="Distribution of SP500 index", breaks = 50) 
points(density(SP500_rtn), type="l", col="blue")
```



<hr>

<p>
Now we want to introduce a new assumption based on the empirical data:
</p>
```{r, echo=FALSE}
dygraph(SP500, main="SP500 Close prices") %>%
  dySeries(name="GSPC.Adjusted", label = "Close Price") %>%
  dyEvent("2018-09-21", "Max", labelLoc = "top") %>%
  dyEvent("2018-12-21", "Min", labelLoc = "top") %>%
  dyEvent("2019-08-30", "Return to max", labelLoc = "top") %>%
  dyEvent("2020-02-14", "Max", labelLoc = "top") %>%
  dyEvent("2020-03-20", "Min", labelLoc = "top") %>%
  dyEvent("2020-08-07", "Return to max", labelLoc = "top") %>%
  dyEvent("2021-12-31", "Max", labelLoc = "top") %>%
  dyAxis("y", label = "Prices") %>%
  dyOptions(stackedGraph = TRUE) %>%
  dyRangeSelector(height = 30)
```

<p>
As you can see in the above graph, markets drop quickly but recover the loss in much more time.<br>
This fact leads us to think that when the market decreases, lose a lot in a short time, so in those days/weeks the volatility is very high. While to return at the same maximum level need more positive days/weeks, so positive increments are smaller than negative ones.<br><br>

At this point we can introduce the GJR-GARCH model, this model introduce a new parameter called 'gamma', this parameter is identical to alpha but it multiplies a dummy variable which is equal to zero when the return is positive while it is equal to one when the return is negative.<br>
So we are trying to model the variance during negative periods in a better way.
</p>

<p><br>Define a GJR-GARCH(1,1) model with <b>Skew Student-t distribution assumpution for errors</b><br>
We have decided to keep the sstd assumption for innovations.</p>

```{r, results='hide'}
s <- ugarchspec(mean.model = list(armaOrder = c(0,0)),
                variance.model = list(model = 'gjrGARCH', garchOrder = c(1,1)),
                distribution.model = 'sstd')

m <- ugarchfit(data = GSPC_week_rtn, spec = s)
```
```{r, echo=FALSE}
capture.output(m)[2:21]

```

<p>
As we can see, alpha is irrelevant but we want to focus our attention on gamma, as told before in this model we are trying to analyze variance during negative returns, and as we can see the p-value of this parameter is near to zero so we can conclude that it is significant.<br>
</p>


```{r, echo=FALSE}
capture.output(m)[43:58]

```

<p>Residuals follow a white noise process.</p>

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot(m, which=10)
plot(m, which=11)
```


```{r, echo=FALSE}
capture.output(m)[92:98]

```

<p>
To conclude we can see how good fit the assumption of a Skew Student-t distribution for innovations in this model.
</p>
```{r, echo=FALSE}
plot(m, which=9)
```

<hr></hr>

<p>
To conclude, we have seen three models, first and second are based on simple GARCH(p,q) while the third is based on GJR-GARCH(p,q).<br>
We set the parameters 'p' and 'q' equal to one, but, for a deeper analysis we could try different combinations like GARCH(1,2) or GARCH(2,1) etc. and compare these models using the AIC information.<br>
It might also be interesting to try a train/test approach, in order to make a prediction analysis.
</p>








